{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitremindervenvef57bff50fb74a938c8a1d9eb5d9dff9",
   "display_name": "Python 3.6.9 64-bit ('reminder': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rohan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/rohan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "count = 1\n",
    "\n",
    "def extract_imperatives(reviews): #it takes a list of reviews and returns a list of imperatives\n",
    "    imperatives = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        \n",
    "        sentences = sent_tokenize(review)\n",
    "    \n",
    "        for sent in sentences: \n",
    "            result = nlp.annotate(sent,\n",
    "                            properties={\n",
    "                                'annotators': 'pos',\n",
    "                                'outputFormat': 'json',\n",
    "                                'timeout': 1000,\n",
    "                            })\n",
    "            try:\n",
    "                if \"VB\" in result[\"sentences\"][0][\"tokens\"][0][\"pos\"]:\n",
    "                    imperatives.append(sent)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "    return imperatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reminder(clean_reviews):\n",
    "    words = []\n",
    "\n",
    "    for sentence in clean_reviews:\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        temp = nltk.word_tokenize(sentence)\n",
    "        words.extend(temp)\n",
    "    if len(words) == 0:\n",
    "        return None, None, None\n",
    "    combined_sentence = [\"\"]\n",
    "    combined_sentence[0] = \" \".join(words)\n",
    "    words_vec = bc.encode(words)\n",
    "    sent_vec = bc.encode(combined_sentence)\n",
    "    similarities = cosine_similarity(sent_vec, words_vec)[0]\n",
    "\n",
    "    sorted_list = np.argsort(similarities)[::-1]\n",
    "    words_bert = []\n",
    "\n",
    "    for k in sorted_list:\n",
    "        if words[k] in words_bert or len(words_bert) >= 20:\n",
    "            continue\n",
    "        words_bert.append(words[k])\n",
    "        \n",
    "    words_tfidf = []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    X = vectorizer.fit_transform(clean_reviews)\n",
    "    X = np.array(X.todense())\n",
    "    X = np.mean(X, axis = 0)\n",
    "\n",
    "    X = np.argsort(X)[::-1]\n",
    "\n",
    "    temp = vectorizer.get_feature_names()\n",
    "\n",
    "    for i in X:\n",
    "        words_tfidf.append(temp[i])\n",
    "    clean_reviews_tokens = []\n",
    "\n",
    "    for i in clean_reviews:\n",
    "        clean_reviews_tokens.append(i.split())\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(clean_reviews_tokens)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in clean_reviews_tokens]\n",
    "\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, num_topics = 10, id2word = dictionary, passes = 10, workers = 2)\n",
    "    words_lda = []\n",
    "\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        temp = topic.split('\"')\n",
    "        for i in range(len(temp)):\n",
    "            if i%2 == 0 or temp[i] in words_lda:\n",
    "                continue\n",
    "            words_lda.append(temp[i])\n",
    "    final_list = list(set(words_bert) & set(words_tfidf) & set(words_lda))\n",
    "\n",
    "    return words_tfidf, words_lda, words_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 of 339\n",
      "2 of 339\n",
      "3 of 339\n",
      "4 of 339\n",
      "5 of 339\n",
      "6 of 339\n",
      "7 of 339\n",
      "8 of 339\n",
      "9 of 339\n",
      "10 of 339\n",
      "11 of 339\n",
      "12 of 339\n",
      "13 of 339\n",
      "14 of 339\n",
      "15 of 339\n",
      "16 of 339\n",
      "17 of 339\n",
      "18 of 339\n",
      "19 of 339\n",
      "20 of 339\n",
      "21 of 339\n",
      "22 of 339\n",
      "23 of 339\n",
      "24 of 339\n",
      "25 of 339\n",
      "26 of 339\n",
      "27 of 339\n",
      "28 of 339\n",
      "29 of 339\n",
      "30 of 339\n",
      "31 of 339\n",
      "32 of 339\n",
      "33 of 339\n",
      "34 of 339\n",
      "35 of 339\n",
      "36 of 339\n",
      "37 of 339\n",
      "38 of 339\n",
      "39 of 339\n",
      "40 of 339\n",
      "41 of 339\n",
      "42 of 339\n",
      "43 of 339\n",
      "44 of 339\n",
      "45 of 339\n",
      "46 of 339\n",
      "47 of 339\n",
      "48 of 339\n",
      "49 of 339\n",
      "50 of 339\n",
      "51 of 339\n",
      "52 of 339\n",
      "53 of 339\n",
      "54 of 339\n",
      "55 of 339\n",
      "56 of 339\n",
      "57 of 339\n",
      "58 of 339\n",
      "59 of 339\n",
      "60 of 339\n",
      "61 of 339\n",
      "62 of 339\n",
      "63 of 339\n",
      "64 of 339\n",
      "65 of 339\n",
      "66 of 339\n",
      "67 of 339\n",
      "68 of 339\n",
      "69 of 339\n",
      "70 of 339\n",
      "71 of 339\n",
      "72 of 339\n",
      "73 of 339\n",
      "74 of 339\n",
      "75 of 339\n",
      "76 of 339\n",
      "77 of 339\n",
      "78 of 339\n",
      "79 of 339\n",
      "80 of 339\n",
      "81 of 339\n",
      "82 of 339\n",
      "83 of 339\n",
      "84 of 339\n",
      "85 of 339\n",
      "86 of 339\n",
      "87 of 339\n",
      "88 of 339\n",
      "89 of 339\n",
      "90 of 339\n",
      "91 of 339\n",
      "92 of 339\n",
      "93 of 339\n",
      "94 of 339\n",
      "95 of 339\n",
      "96 of 339\n",
      "97 of 339\n",
      "98 of 339\n",
      "99 of 339\n",
      "100 of 339\n",
      "101 of 339\n",
      "102 of 339\n",
      "103 of 339\n",
      "104 of 339\n",
      "105 of 339\n",
      "106 of 339\n",
      "107 of 339\n",
      "108 of 339\n",
      "109 of 339\n",
      "110 of 339\n",
      "111 of 339\n",
      "112 of 339\n",
      "113 of 339\n",
      "114 of 339\n",
      "115 of 339\n",
      "116 of 339\n",
      "117 of 339\n",
      "118 of 339\n",
      "119 of 339\n",
      "120 of 339\n",
      "121 of 339\n",
      "122 of 339\n",
      "123 of 339\n",
      "124 of 339\n",
      "125 of 339\n",
      "126 of 339\n",
      "127 of 339\n",
      "128 of 339\n",
      "129 of 339\n",
      "130 of 339\n",
      "131 of 339\n",
      "132 of 339\n",
      "133 of 339\n",
      "134 of 339\n",
      "135 of 339\n",
      "136 of 339\n",
      "137 of 339\n",
      "138 of 339\n",
      "139 of 339\n",
      "140 of 339\n",
      "141 of 339\n",
      "142 of 339\n",
      "143 of 339\n",
      "144 of 339\n",
      "145 of 339\n",
      "146 of 339\n",
      "147 of 339\n",
      "148 of 339\n",
      "149 of 339\n",
      "150 of 339\n",
      "151 of 339\n",
      "152 of 339\n",
      "153 of 339\n",
      "154 of 339\n",
      "155 of 339\n",
      "156 of 339\n",
      "157 of 339\n",
      "158 of 339\n",
      "159 of 339\n",
      "160 of 339\n",
      "161 of 339\n",
      "162 of 339\n",
      "163 of 339\n",
      "164 of 339\n",
      "165 of 339\n",
      "166 of 339\n",
      "167 of 339\n",
      "168 of 339\n",
      "169 of 339\n",
      "170 of 339\n",
      "171 of 339\n",
      "172 of 339\n",
      "173 of 339\n",
      "174 of 339\n",
      "175 of 339\n",
      "176 of 339\n",
      "177 of 339\n",
      "178 of 339\n",
      "179 of 339\n",
      "180 of 339\n",
      "181 of 339\n",
      "182 of 339\n",
      "183 of 339\n",
      "184 of 339\n",
      "185 of 339\n",
      "186 of 339\n",
      "187 of 339\n",
      "188 of 339\n",
      "189 of 339\n",
      "190 of 339\n",
      "191 of 339\n",
      "192 of 339\n",
      "193 of 339\n",
      "194 of 339\n",
      "195 of 339\n",
      "196 of 339\n",
      "197 of 339\n",
      "198 of 339\n",
      "199 of 339\n",
      "200 of 339\n",
      "201 of 339\n",
      "202 of 339\n",
      "203 of 339\n",
      "204 of 339\n",
      "205 of 339\n",
      "206 of 339\n",
      "207 of 339\n",
      "208 of 339\n",
      "209 of 339\n",
      "210 of 339\n",
      "211 of 339\n",
      "212 of 339\n",
      "213 of 339\n",
      "214 of 339\n",
      "215 of 339\n",
      "216 of 339\n",
      "217 of 339\n",
      "218 of 339\n",
      "219 of 339\n",
      "220 of 339\n",
      "221 of 339\n",
      "222 of 339\n",
      "223 of 339\n",
      "224 of 339\n",
      "225 of 339\n",
      "226 of 339\n",
      "227 of 339\n",
      "228 of 339\n",
      "229 of 339\n",
      "230 of 339\n",
      "231 of 339\n",
      "232 of 339\n",
      "233 of 339\n",
      "234 of 339\n",
      "235 of 339\n",
      "236 of 339\n",
      "237 of 339\n",
      "238 of 339\n",
      "239 of 339\n",
      "240 of 339\n",
      "241 of 339\n",
      "242 of 339\n",
      "243 of 339\n",
      "244 of 339\n",
      "245 of 339\n",
      "246 of 339\n",
      "247 of 339\n",
      "248 of 339\n",
      "249 of 339\n",
      "250 of 339\n",
      "251 of 339\n",
      "252 of 339\n",
      "253 of 339\n",
      "254 of 339\n",
      "255 of 339\n",
      "256 of 339\n",
      "257 of 339\n",
      "258 of 339\n",
      "259 of 339\n",
      "260 of 339\n",
      "261 of 339\n",
      "262 of 339\n",
      "263 of 339\n",
      "264 of 339\n",
      "265 of 339\n",
      "266 of 339\n",
      "267 of 339\n",
      "268 of 339\n",
      "269 of 339\n",
      "270 of 339\n",
      "271 of 339\n",
      "272 of 339\n",
      "273 of 339\n",
      "274 of 339\n",
      "275 of 339\n",
      "276 of 339\n",
      "277 of 339\n",
      "278 of 339\n",
      "279 of 339\n",
      "280 of 339\n",
      "281 of 339\n",
      "282 of 339\n",
      "283 of 339\n",
      "284 of 339\n",
      "285 of 339\n",
      "286 of 339\n",
      "287 of 339\n",
      "288 of 339\n",
      "289 of 339\n",
      "290 of 339\n",
      "291 of 339\n",
      "292 of 339\n",
      "293 of 339\n",
      "294 of 339\n",
      "295 of 339\n",
      "296 of 339\n",
      "297 of 339\n",
      "298 of 339\n",
      "299 of 339\n",
      "300 of 339\n",
      "301 of 339\n",
      "302 of 339\n",
      "303 of 339\n",
      "304 of 339\n",
      "305 of 339\n",
      "306 of 339\n",
      "307 of 339\n",
      "308 of 339\n",
      "309 of 339\n",
      "310 of 339\n",
      "311 of 339\n",
      "312 of 339\n",
      "313 of 339\n",
      "314 of 339\n",
      "315 of 339\n",
      "316 of 339\n",
      "317 of 339\n",
      "318 of 339\n",
      "319 of 339\n",
      "320 of 339\n",
      "321 of 339\n",
      "322 of 339\n",
      "323 of 339\n",
      "324 of 339\n",
      "325 of 339\n",
      "326 of 339\n",
      "327 of 339\n",
      "328 of 339\n",
      "329 of 339\n",
      "330 of 339\n",
      "331 of 339\n",
      "332 of 339\n",
      "333 of 339\n",
      "334 of 339\n",
      "335 of 339\n",
      "336 of 339\n",
      "337 of 339\n",
      "338 of 339\n",
      "339 of 339\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "count = 1\n",
    "\n",
    "\n",
    "for file_name in os.listdir('data/Recent'):\n",
    "    print(count, 'of', len(os.listdir('data/Recent')))\n",
    "    count += 1\n",
    "    if file_name[:-4]+'_bert.csv' in os.listdir('bert_lda_tfidf'):\n",
    "        continue\n",
    "    df = pd.read_csv('data/Recent/' + file_name)\n",
    "    reviews = df.iloc[:, 1].values\n",
    "    imperatives = extract_imperatives(reviews)\n",
    "    clean_imperatives = list(map(clean_text, imperatives))\n",
    "    words_tfidf, words_lda, words_bert = reminder(clean_imperatives)\n",
    "    if words_lda == None:\n",
    "        continue\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    df3 = pd.DataFrame()\n",
    "    \n",
    "    df1['bert'] = words_bert\n",
    "    df2['tfidf'] = words_tfidf\n",
    "    df3['lda'] = words_lda\n",
    "\n",
    "    df1.to_csv('bert_lda_tfidf/'+file_name[:-4]+'_bert.csv', index=False)\n",
    "    df2.to_csv('bert_lda_tfidf/'+file_name[:-4]+'_lda.csv', index=False)\n",
    "    df3.to_csv('bert_lda_tfidf/'+file_name[:-4]+'_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}